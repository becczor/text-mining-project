{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias in word embeddings\n",
    "Word embeddings is a technique in NLP and text mining to represent words in order to be able to compare words based on similarity. The word embeddings are trained on large data sets with text written by humans. Therefore, the bias we have and include in our writungs will be transferred to the word embeddings. This project looks at what bias there are in the swedish word embeddings \"Swectors\" based on text from Göteborgsposten. They will also be compared with another set of word ebedding that will be trained on antoher set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing datasets\n",
    "Importing the large set of vectors might take a little while, please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = [\"word\"] + [\"dim\" + str(x) for x in range(1,301)]\n",
    "\n",
    "with bz2.open(\"swectors-300dim.txt.bz2\") as source:\n",
    "    swectors = pd.read_csv(source, header=None, names=colnames, delimiter=\" \", skiprows=[0])\n",
    "    \n",
    "with bz2.open(\"becctors-300dim.txt.bz2\") as source:\n",
    "    becctors = pd.read_csv(source, header=None, names=colnames, delimiter=\" \", skiprows=[0])\n",
    "    \n",
    "#swectors.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import small sized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colnames = [\"word\"] + [\"dim\" + str(x) for x in range(1,301)]\n",
    "with bz2.open(\"swectors_short-300dim.txt.bz2\") as source:\n",
    "    swectors_short = pd.read_csv(source, header=None, names=colnames, delimiter=\" \", skiprows=[0])\n",
    "    \n",
    "swectors_short.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the vector for the word 'kvinna', in order to look att similar words for bias measures.\n",
    "Get the 300 dimensions from the dataframe, convert to numpy and get the following format: `[[dim1 dim2 ... dim299 dim300]]`, take the first element to get a single list. Save it as a tuple with word first and vector second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvinna_s = ('kvinna', swectors.loc[swectors['word'] == 'kvinna'].loc[:, 'dim1':'dim300'].to_numpy()[0])\n",
    "man_s = ('man', swectors.loc[swectors['word'] == 'man'].loc[:, 'dim1':'dim300'].to_numpy()[0])\n",
    "\n",
    "kvinna_b = ('kvinna', becctors.loc[becctors['word'] == 'kvinna'].loc[:, 'dim1':'dim300'].to_numpy()[0])\n",
    "man_b = ('man', becctors.loc[becctors['word'] == 'man'].loc[:, 'dim1':'dim300'].to_numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(word1, word2):\n",
    "    # Takes two vectors and calculates the cosine similarity between them\n",
    "    # @ is dot product\n",
    "    v1 = word1[1]\n",
    "    v2 = word2[1]\n",
    "    return (v1 @ v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3283947788176963"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(kvinna_s, man_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying function to whole dataframe\n",
    "This way of doing the calculation will apply a function to each row of the dataframe, and return a Series of same length with all results in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_most_similar(vectors, n, word, vectors_filtered=None):\n",
    "    \n",
    "    if vectors_filtered is not None:\n",
    "        df = vectors_filtered\n",
    "        print(\"filtered\")\n",
    "    else:\n",
    "        df = vectors\n",
    "        print(\"not filtered\")\n",
    "        \n",
    "    word_vec = (word, vectors.loc[vectors['word'] == word].loc[:, 'dim1':'dim300'].to_numpy()[0])\n",
    "    \n",
    "    def similarity(row):\n",
    "        row_vec = (row['word'], row.loc['dim1':'dim300'].to_numpy())\n",
    "        return cosine_similarity(word_vec, row_vec)\n",
    "\n",
    "    start = time.time()\n",
    "    similarities = df.apply(similarity, axis=1)\n",
    "\n",
    "    # Concatenate the top n words (plus the word itself) to the similarity values of each word.\n",
    "    # Also set the correct coulmn name.\n",
    "    s1 = df.loc[similarities.nlargest(n+1).index, 'word']\n",
    "    s2 = similarities.nlargest(n+1)\n",
    "    similars = pd.concat([s1, s2], axis=1).rename(columns={0: \"similarity\"})\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed: \", end - start)\n",
    "    return similars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not filtered\n",
      "Time elapsed:  36.43729591369629\n",
      "           word  similarity\n",
      "588     kvinnan    1.000000\n",
      "303      mannen    0.865082\n",
      "1829    flickan    0.843521\n",
      "2486     mamman    0.750042\n",
      "2516     pojken    0.744725\n",
      "4381     offret    0.719690\n",
      "2775     pappan    0.679648\n",
      "493      kvinna    0.679172\n",
      "31          hon    0.664614\n",
      "1077     männen    0.662218\n",
      "34234  vårdaren    0.652844\n"
     ]
    }
   ],
   "source": [
    "similars_kvinna_s = n_most_similar(swectors, 10, 'kvinnan')\n",
    "print(similars_kvinna_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not filtered\n",
      "Time elapsed:  29.458340406417847\n",
      "            word  similarity\n",
      "1247     kvinnan    1.000000\n",
      "5635    kvinnans    0.660167\n",
      "723       kvinna    0.631024\n",
      "2500     flickan    0.624874\n",
      "2021      mamman    0.582821\n",
      "523       mannen    0.569707\n",
      "522      kvinnor    0.568113\n",
      "3443   kvinnorna    0.565799\n",
      "14527    hustrun    0.541186\n",
      "971       tjejen    0.526948\n",
      "4226       damen    0.517224\n"
     ]
    }
   ],
   "source": [
    "similars_kvinna_b = n_most_similar(becctors, 10, 'kvinnan')\n",
    "print(similars_kvinna_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not filtered\n",
      "Time elapsed:  36.482909202575684\n",
      "                 word  similarity\n",
      "303            mannen    1.000000\n",
      "588           kvinnan    0.865082\n",
      "2516           pojken    0.824330\n",
      "9286      polismannen    0.760309\n",
      "1829          flickan    0.751662\n",
      "4506   gärningsmannen    0.724007\n",
      "19807       ynglingen    0.708831\n",
      "4381           offret    0.707234\n",
      "2775           pappan    0.689243\n",
      "16049          vakten    0.685423\n",
      "18040      målsägaren    0.676460\n"
     ]
    }
   ],
   "source": [
    "similars_män_s = n_most_similar(swectors, 10, 'mannen')\n",
    "print(similars_män_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not filtered\n",
      "Time elapsed:  26.49137544631958\n",
      "         word  similarity\n",
      "523    mannen    1.000000\n",
      "914     maken    0.741162\n",
      "525    sambon    0.700235\n",
      "759    killen    0.671338\n",
      "1480   gubben    0.659938\n",
      "4075    karln    0.611537\n",
      "5725     frun    0.608822\n",
      "32        han    0.605173\n",
      "1247  kvinnan    0.569707\n",
      "2297   pappan    0.562663\n",
      "175     honom    0.557921\n"
     ]
    }
   ],
   "source": [
    "similars_män_b = n_most_similar(becctors, 10, 'mannen')\n",
    "print(similars_män_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering out all adjectives in the word embeddings\n",
    "To see bias in adjectives, the dataframe with the swectors is filtered to only keep words that are in the dataframe with adjectves from Språkrådet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open(\"adjektiv.txt.bz2\") as source:\n",
    "    adjectives = pd.read_csv(source)\n",
    "    \n",
    "swectors_filtered = swectors.loc[swectors['word'].isin(adjectives['Word'])]\n",
    "\n",
    "becctors_filtered = becctors.loc[becctors['word'].isin(adjectives['Word'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swectors_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered\n",
      "Time elapsed:  3.014665126800537\n",
      "                     word  similarity\n",
      "22352              bärbar    0.500918\n",
      "15547             bärbara    0.491010\n",
      "13986            digitalt    0.482039\n",
      "21152            trådlöst    0.477740\n",
      "109616          sladdlösa    0.452600\n",
      "26434            trådlösa    0.433673\n",
      "72740            portabla    0.404584\n",
      "34138             trådlös    0.403282\n",
      "12732        elektroniskt    0.399799\n",
      "49645   barnpornografiska    0.395040\n",
      "80051           stationär    0.394396\n"
     ]
    }
   ],
   "source": [
    "similars_kvinna_adj_s = n_most_similar(swectors, 10, 'dator', swectors_filtered)\n",
    "print(similars_kvinna_adj_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered\n",
      "Time elapsed:  1.7819604873657227\n",
      "                word  similarity\n",
      "14512        externa    0.428106\n",
      "25263         mobilt    0.382808\n",
      "35398       portabel    0.299629\n",
      "86820     okrypterad    0.294558\n",
      "119102    okrypterat    0.290431\n",
      "7494        manuellt    0.283486\n",
      "81643      urkopplad    0.280656\n",
      "22224   elektroniskt    0.274290\n",
      "5324          trasig    0.268814\n",
      "23956         mobila    0.263520\n",
      "111032   okrypterade    0.262835\n"
     ]
    }
   ],
   "source": [
    "similars_kvinna_adj_b = n_most_similar(becctors, 10, 'dator', becctors_filtered)\n",
    "print(similars_kvinna_adj_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
